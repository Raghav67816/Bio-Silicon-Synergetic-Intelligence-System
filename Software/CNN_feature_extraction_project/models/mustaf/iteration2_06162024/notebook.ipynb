{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b15837c4-2809-42db-8c51-c5e878e97f77",
   "metadata": {},
   "source": [
    "# 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "220671a0-e15e-44c3-8f84-a2d195770dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Dataset Size: 444\n",
      "Inputs: tensor([[[0.0057],\n",
      "         [0.0054],\n",
      "         [0.0045],\n",
      "         ...,\n",
      "         [0.0045],\n",
      "         [0.0140],\n",
      "         [0.0030]],\n",
      "\n",
      "        [[0.0026],\n",
      "         [0.0071],\n",
      "         [0.0024],\n",
      "         ...,\n",
      "         [0.0064],\n",
      "         [0.0056],\n",
      "         [0.0088]],\n",
      "\n",
      "        [[0.0070],\n",
      "         [0.0050],\n",
      "         [0.0067],\n",
      "         ...,\n",
      "         [0.0029],\n",
      "         [0.0030],\n",
      "         [0.0032]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0064],\n",
      "         [0.0059],\n",
      "         [0.0040],\n",
      "         ...,\n",
      "         [0.0017],\n",
      "         [0.0049],\n",
      "         [0.0022]],\n",
      "\n",
      "        [[0.0075],\n",
      "         [0.0095],\n",
      "         [0.0021],\n",
      "         ...,\n",
      "         [0.0073],\n",
      "         [0.0063],\n",
      "         [0.0077]],\n",
      "\n",
      "        [[0.0136],\n",
      "         [0.0089],\n",
      "         [0.0010],\n",
      "         ...,\n",
      "         [0.0035],\n",
      "         [0.0050],\n",
      "         [0.0096]]])\n",
      "Labels: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Specify the paths\n",
    "data_dir = '../../../datasets/processed/doom/'\n",
    "\n",
    "# Load the tensors\n",
    "variance_tensor = torch.load(os.path.join(data_dir, 'variance_250ms_500hz_tensor.pt'))\n",
    "labels_tensor = torch.load(os.path.join(data_dir, 'variance_labels_250ms_500hz_tensor.pt'))\n",
    "\n",
    "# Create the TensorDataset\n",
    "variance_dataset = torch.utils.data.TensorDataset(variance_tensor, labels_tensor)\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "train_size = int(0.7 * len(variance_dataset))\n",
    "val_size = int(0.15 * len(variance_dataset))\n",
    "test_size = len(variance_dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(variance_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "i = 0\n",
    "# You can now use the DataLoaders in your training loop\n",
    "for inputs, labels in train_loader:\n",
    "    # Your training loop here\n",
    "    print(\"Total Dataset Size:\", len(variance_tensor))\n",
    "    print(\"Inputs:\", inputs)\n",
    "    print(\"Labels:\", labels)\n",
    "    i=i+1\n",
    "    if i == 1:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8ed1be-13bf-452a-b135-72b81bab8821",
   "metadata": {},
   "source": [
    "# Check Shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac513992-7170-4b92-92fe-a084e88241ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance Tensor Shape: torch.Size([444, 32, 1])\n",
      "Variance Tensor Contents:\n",
      "tensor([[[0.0021],\n",
      "         [0.0057],\n",
      "         [0.0020],\n",
      "         ...,\n",
      "         [0.0020],\n",
      "         [0.0030],\n",
      "         [0.0030]],\n",
      "\n",
      "        [[0.0024],\n",
      "         [0.0058],\n",
      "         [0.0016],\n",
      "         ...,\n",
      "         [0.0023],\n",
      "         [0.0017],\n",
      "         [0.0013]],\n",
      "\n",
      "        [[0.0050],\n",
      "         [0.0069],\n",
      "         [0.0044],\n",
      "         ...,\n",
      "         [0.0017],\n",
      "         [0.0071],\n",
      "         [0.0021]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0139],\n",
      "         [0.0063],\n",
      "         [0.0062],\n",
      "         ...,\n",
      "         [0.0017],\n",
      "         [0.0021],\n",
      "         [0.0073]],\n",
      "\n",
      "        [[0.0084],\n",
      "         [0.0065],\n",
      "         [0.0106],\n",
      "         ...,\n",
      "         [0.0011],\n",
      "         [0.0066],\n",
      "         [0.0018]],\n",
      "\n",
      "        [[0.0040],\n",
      "         [0.0071],\n",
      "         [0.0068],\n",
      "         ...,\n",
      "         [0.0058],\n",
      "         [0.0020],\n",
      "         [0.0052]]])\n",
      "Labels Tensor Shape: torch.Size([444])\n",
      "Labels Tensor Contents:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# Print the shape and contents of the variance tensor\n",
    "print(\"Variance Tensor Shape:\", variance_tensor.shape)\n",
    "print(\"Variance Tensor Contents:\")\n",
    "print(variance_tensor)\n",
    "\n",
    "# Print the shape and contents of the labels tensor\n",
    "print(\"Labels Tensor Shape:\", labels_tensor.shape)\n",
    "print(\"Labels Tensor Contents:\")\n",
    "print(labels_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d4aa7f-0075-4fb6-aea2-097f9eaea4bd",
   "metadata": {},
   "source": [
    "# Define Custom Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25ca0870-e9fe-4d89-a81a-4c873e6ee985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PeakHeightActivation(nn.Module):\n",
    "    def __init__(self, filepath):\n",
    "        super(PeakHeightActivation, self).__init__()\n",
    "        self.peak_height_values = np.memmap(filepath, dtype='float32', mode='r')\n",
    "\n",
    "    def forward(self, x):\n",
    "        peak_height = torch.tensor(self.peak_height_values, device=x.device)\n",
    "        return x * peak_height  # Scale by peak height\n",
    "\n",
    "class PeakCountsActivation(nn.Module):\n",
    "    def __init__(self, filepath):\n",
    "        super(PeakCountsActivation, self).__init__()\n",
    "        self.peak_counts_values = np.memmap(filepath, dtype='float32', mode='r')\n",
    "\n",
    "    def forward(self, x):\n",
    "        peak_counts = torch.tensor(self.peak_counts_values, device=x.device)\n",
    "        return x * peak_counts  # Scale by peak counts\n",
    "\n",
    "class AveragePeakHeightActivation(nn.Module):\n",
    "    def __init__(self, filepath):\n",
    "        super(AveragePeakHeightActivation, self).__init__()\n",
    "        self.average_peak_height_values = np.memmap(filepath, dtype='float32', mode='r')\n",
    "\n",
    "    def forward(self, x):\n",
    "        average_peak_height = torch.tensor(self.average_peak_height_values, device=x.device)\n",
    "        return x * average_peak_height  # Scale by average peak height\n",
    "\n",
    "class AverageDistanceActivation(nn.Module):\n",
    "    def __init__(self, filepath):\n",
    "        super(AverageDistanceActivation, self).__init__()\n",
    "        self.average_distance_values = np.memmap(filepath, dtype='float32', mode='r')\n",
    "\n",
    "    def forward(self, x):\n",
    "        average_distance = torch.tensor(self.average_distance_values, device=x.device)\n",
    "        return x * average_distance  # Scale by average distance\n",
    "\n",
    "class AverageProminenceActivation(nn.Module):\n",
    "    def __init__(self, filepath):\n",
    "        super(AverageProminenceActivation, self).__init__()\n",
    "        self.prominence_values = np.memmap(filepath, dtype='float32', mode='r')\n",
    "\n",
    "    def forward(self, x):\n",
    "        prominence = torch.tensor(self.prominence_values, device=x.device)\n",
    "        return x * prominence  # Scale by prominence\n",
    "\n",
    "class VarianceActivation(nn.Module):\n",
    "    def __init__(self, filepath):\n",
    "        super(VarianceActivation, self).__init__()\n",
    "        self.variance_values = np.memmap(filepath, dtype='float32', mode='r')\n",
    "\n",
    "    def forward(self, x):\n",
    "        variance = torch.tensor(self.variance_values, device=x.device)\n",
    "        return x * variance  # Scale by variance\n",
    "\n",
    "class StdDevActivation(nn.Module):\n",
    "    def __init__(self, filepath):\n",
    "        super(StdDevActivation, self).__init__()\n",
    "        self.std_dev_values = np.memmap(filepath, dtype='float32', mode='r')\n",
    "\n",
    "    def forward(self, x):\n",
    "        std_dev = torch.tensor(self.std_dev_values, device=x.device)\n",
    "        return x * std_dev  # Scale by standard deviation\n",
    "\n",
    "class RMSActivation(nn.Module):\n",
    "    def __init__(self, filepath):\n",
    "        super(RMSActivation, self).__init__()\n",
    "        self.rms_values = np.memmap(filepath, dtype='float32', mode='r')\n",
    "\n",
    "    def forward(self, x):\n",
    "        rms = torch.tensor(self.rms_values, device=x.device)\n",
    "        return x * rms  # Scale by RMS\n",
    "\n",
    "class FrequenciesActivation(nn.Module):\n",
    "    def __init__(self, filepath):\n",
    "        super(FrequenciesActivation, self).__init__()\n",
    "        self.frequencies_values = np.memmap(filepath, dtype='float32', mode='r')\n",
    "\n",
    "    def forward(self, x):\n",
    "        frequencies = torch.tensor(self.frequencies_values, device=x.device)\n",
    "        return x * frequencies  # Scale by frequencies\n",
    "\n",
    "class PSDActivation(nn.Module):\n",
    "    def __init__(self, filepath):\n",
    "        super(PSDActivation, self).__init__()\n",
    "        self.psd_values = np.memmap(filepath, dtype='float32', mode='r')\n",
    "\n",
    "    def forward(self, x):\n",
    "        psd = torch.tensor(self.psd_values, device=x.device)\n",
    "        return x * psd  # Scale by PSD\n",
    "\n",
    "class SpectralEntropyActivation(nn.Module):\n",
    "    def __init__(self, filepath):\n",
    "        super(SpectralEntropyActivation, self).__init__()\n",
    "        self.spectral_entropy_values = np.memmap(filepath, dtype='float32', mode='r')\n",
    "\n",
    "    def forward(self, x):\n",
    "        spectral_entropy = torch.tensor(self.spectral_entropy_values, device=x.device)\n",
    "        return x * spectral_entropy  # Scale by spectral entropy\n",
    "\n",
    "class FFTResultsActivation(nn.Module):\n",
    "    def __init__(self, filepath):\n",
    "        super(FFTResultsActivation, self).__init__()\n",
    "        self.fft_results_values = np.memmap(filepath, dtype='float32', mode='r')\n",
    "\n",
    "    def forward(self, x):\n",
    "        fft_results = torch.tensor(self.fft_results_values, device=x.device)\n",
    "        return x * fft_results  # Scale by FFT results\n",
    "\n",
    "class MagnitudesActivation(nn.Module):\n",
    "    def __init__(self, filepath):\n",
    "        super(MagnitudesActivation, self).__init__()\n",
    "        self.magnitudes_values = np.memmap(filepath, dtype='float32', mode='r')\n",
    "\n",
    "    def forward(self, x):\n",
    "        magnitudes = torch.tensor(self.magnitudes_values, device=x.device)\n",
    "        return x * magnitudes  # Scale by magnitudes\n",
    "\n",
    "class CentroidsActivation(nn.Module):\n",
    "    def __init__(self, filepath):\n",
    "        super(CentroidsActivation, self).__init__()\n",
    "        self.centroids_values = np.memmap(filepath, dtype='float32', mode='r')\n",
    "\n",
    "    def forward(self, x):\n",
    "        centroids = torch.tensor(self.centroids_values, device=x.device)\n",
    "        return x * centroids  # Scale by centroids\n",
    "\n",
    "class SpectralEdgeDensityActivation(nn.Module):\n",
    "    def __init__(self, filepath):\n",
    "        super(SpectralEdgeDensityActivation, self).__init__()\n",
    "        self.spectral_edge_density_values = np.memmap(filepath, dtype='float32', mode='r')\n",
    "\n",
    "    def forward(self, x):\n",
    "        spectral_edge_density = torch.tensor(self.spectral_edge_density_values, device=x.device)\n",
    "        return x * spectral_edge_density  # Scale by spectral edge density\n",
    "\n",
    "class PositiveFrequenciesActivation(nn.Module):\n",
    "    def __init__(self, filepath):\n",
    "        super(PositiveFrequenciesActivation, self).__init__()\n",
    "        self.positive_frequencies_values = np.memmap(filepath, dtype='float32', mode='r')\n",
    "\n",
    "    def forward(self, x):\n",
    "        positive_frequencies = torch.tensor(self.positive_frequencies_values, device=x.device)\n",
    "        return x * positive_frequencies  # Scale by positive frequencies\n",
    "\n",
    "class PositiveFFTResultsActivation(nn.Module):\n",
    "    def __init__(self, filepath):\n",
    "        super(PositiveFFTResultsActivation, self).__init__()\n",
    "        self.positive_fft_results_values = np.memmap(filepath, dtype='float32', mode='r')\n",
    "\n",
    "    def forward(self, x):\n",
    "        positive_fft_results = torch.tensor(self.positive_fft_results_values, device=x.device)\n",
    "        return x * positive_fft_results  # Scale by positive FFT results\n",
    "\n",
    "class CumulativeSumsActivation(nn.Module):\n",
    "    def __init__(self, filepath):\n",
    "        super(CumulativeSumsActivation, self).__init__()\n",
    "        self.cumulative_sums_values = np.memmap(filepath, dtype='float32', mode='r')\n",
    "\n",
    "    def forward(self, x):\n",
    "        cumulative_sums = torch.tensor(self.cumulative_sums_values, device=x.device)\n",
    "        return x * cumulative_sums  # Scale by cumulative sums\n",
    "\n",
    "class TotalPowersActivation(nn.Module):\n",
    "    def __init__(self, filepath):\n",
    "        super(TotalPowersActivation, self).__init__()\n",
    "        self.total_powers_values = np.memmap(filepath, dtype='float32', mode='r')\n",
    "\n",
    "    def forward(self, x):\n",
    "        total_powers = torch.tensor(self.total_powers_values, device=x.device)\n",
    "        return x * total_powers  # Scale by total powers\n",
    "\n",
    "class ThresholdsActivation(nn.Module):\n",
    "    def __init__(self, filepath):\n",
    "        super(ThresholdsActivation, self).__init__()\n",
    "        self.thresholds_values = np.memmap(filepath, dtype='float32', mode='r')\n",
    "\n",
    "    def forward(self, x):\n",
    "        threshold = torch.tensor(self.thresholds_values, device=x.device)\n",
    "        return x * threshold  # Scale by thresholds\n",
    "\n",
    "class PhasesActivation(nn.Module):\n",
    "    def __init__(self, filepath):\n",
    "        super(PhasesActivation, self).__init__()\n",
    "        self.phases_values = np.memmap(filepath, dtype='float32', mode='r')\n",
    "\n",
    "    def forward(self, x):\n",
    "        phases = torch.tensor(self.phases_values, device=x.device)\n",
    "        return x * phases  # Scale by phases\n",
    "\n",
    "class PairwisePhaseLockingValuesActivation(nn.Module):\n",
    "    def __init__(self, filepath):\n",
    "        super(PairwisePhaseLockingValuesActivation, self).__init__()\n",
    "        self.pairwise_phase_locking_values_values = np.memmap(filepath, dtype='float32', mode='r')\n",
    "\n",
    "    def forward(self, x):\n",
    "        pairwise_phase_locking_values = torch.tensor(self.pairwise_phase_locking_values_values, device=x.device)\n",
    "        return x * pairwise_phase_locking_values  # Scale by pairwise phase locking values\n",
    "\n",
    "class HiguchiFractalDimensionActivation(nn.Module):\n",
    "    def __init__(self, filepath):\n",
    "        super(HiguchiFractalDimensionActivation, self).__init__()\n",
    "        self.hfd_values = np.memmap(filepath, dtype='float32', mode='r')\n",
    "\n",
    "    def forward(self, x):\n",
    "        hfd_values = torch.tensor(self.hfd_values, device=x.device)\n",
    "        return x * hfd_values  # Scale by Higuchi fractal dimension values\n",
    "       \n",
    "class PhaseSpaceActivation(nn.Module):\n",
    "    def __init__(self, filepath):\n",
    "        super(PhaseSpaceActivation, self).__init__()\n",
    "        self.phase_space_values = np.memmap(filepath, dtype='float32', mode='r')\n",
    "\n",
    "    def forward(self, x):\n",
    "        phase_space = torch.tensor(self.phase_space_values, device=x.device)\n",
    "        return phase_space\n",
    "\n",
    "class UMAPActivation(nn.Module):\n",
    "    def __init__(self, filepath):\n",
    "        super(UMAPActivation, self).__init__()\n",
    "        self.umap_values = np.memmap(filepath, dtype='float32', mode='r')\n",
    "\n",
    "    def forward(self, x):\n",
    "        umap = torch.tensor(self.umap_values, device=x.device)\n",
    "        return umap\n",
    "\n",
    "class TSNEActivation(nn.Module):\n",
    "    def __init__(self, filepath):\n",
    "        super(TSNEActivation, self).__init__()\n",
    "        self.tsne_values = np.memmap(filepath, dtype='float32', mode='r')\n",
    "\n",
    "    def forward(self, x):\n",
    "        tsne = torch.tensor(self.tsne_values, device=x.device)\n",
    "        return tsne\n",
    "\n",
    "class SampleEntropyActivation(nn.Module):\n",
    "    def __init__(self, filepath):\n",
    "        super(SampleEntropyActivation, self).__init__()\n",
    "        self.sample_entropy_values = np.memmap(filepath, dtype='float32', mode='r')\n",
    "\n",
    "    def forward(self, x):\n",
    "        sample_entropy = torch.tensor(self.sample_entropy_values, device=x.device)\n",
    "        return sample_entropy\n",
    "\n",
    "class BoxCountingDimensionActivation(nn.Module):\n",
    "    def __init__(self, filepath):\n",
    "        super(BoxCountingDimensionActivation, self).__init__()\n",
    "        self.box_counting_dimension_values = np.memmap(filepath, dtype='float32', mode='r')\n",
    "\n",
    "    def forward(self, x):\n",
    "        box_counting_dimension = torch.tensor(self.box_counting_dimension_values, device=x.device)\n",
    "        return box_counting_dimension\n",
    "\n",
    "class TopologicalActivation(nn.Module):\n",
    "    def __init__(self, filepath):\n",
    "        super(TopologicalActivation, self).__init__()\n",
    "        self.topological_values = np.memmap(filepath, dtype='float32', mode='r')\n",
    "\n",
    "    def forward(self, x):\n",
    "        topological = torch.tensor(self.topological_values, device=x.device)\n",
    "        return topological\n",
    "\n",
    "class LyapunovActivation(nn.Module):\n",
    "    def __init__(self, filepath):\n",
    "        super(LyapunovActivation, self).__init__()\n",
    "        self.lyapunov_values = np.memmap(filepath, dtype='float32', mode='r')\n",
    "\n",
    "    def forward(self, x):\n",
    "        lyapunov = torch.tensor(self.lyapunov_values, device=x.device)\n",
    "        return lyapunov\n",
    "\n",
    "class KatzFractalDimensionActivation(nn.Module):\n",
    "    def __init__(self, filepath):\n",
    "        super(KatzFractalDimensionActivation, self).__init__()\n",
    "        self.katz_fractal_dimension_values = np.memmap(filepath, dtype='float32', mode='r')\n",
    "\n",
    "    def forward(self, x):\n",
    "        katz_fractal_dimension = torch.tensor(self.katz_fractal_dimension_values, device=x.device)\n",
    "        return katz_fractal_dimension\n",
    "\n",
    "class MultiscaleEntropyActivation(nn.Module):\n",
    "    def __init__(self, filepath):\n",
    "        super(MultiscaleEntropyActivation, self).__init__()\n",
    "        self.multiscale_entropy_values = np.memmap(filepath, dtype='float32', mode='r')\n",
    "\n",
    "    def forward(self, x):\n",
    "        multiscale_entropy = torch.tensor(self.multiscale_entropy_values, device=x.device)\n",
    "        return multiscale_entropy\n",
    "\n",
    "class WaveletFractalActivation(nn.Module):\n",
    "    def __init__(self, filepath):\n",
    "        super(WaveletFractalActivation, self).__init__()\n",
    "        self.wavelet_fractal_values = np.memmap(filepath, dtype='float32', mode='r')\n",
    "\n",
    "    def forward(self, x):\n",
    "        wavelet_fractal = torch.tensor(self.wavelet_fractal_values, device=x.device)\n",
    "        return wavelet_fractal\n",
    "\n",
    "\n",
    "# need to fix the rest of these for memmap npy\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "class ApproximateEntropyActivation(nn.Module):\n",
    "    def __init__(self, m=2, r=0.2):\n",
    "        super(ApproximateEntropyActivation, self).__init__()\n",
    "        self.m = m\n",
    "        self.r = r\n",
    "\n",
    "    def forward(self, x):\n",
    "        N = x.size(1)\n",
    "        phi_m = torch.mean(torch.abs(x[:, None, :N - self.m] - x[:, None, self.m:N]))\n",
    "        phi_m_plus_one = torch.mean(torch.abs(x[:, None, :N - self.m - 1] - x[:, None, self.m + 1:N]))\n",
    "        return torch.log(phi_m / (phi_m_plus_one + 1e-9))\n",
    "\n",
    "class NetworkFractalDimensionActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetworkFractalDimensionActivation, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        adjacency_matrix = (x @ x.T) > 0.5\n",
    "        return torch.log(torch.sum(adjacency_matrix.float())) / torch.log(torch.tensor(adjacency_matrix.size(-1)).float())\n",
    "\n",
    "class HamiltonianActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HamiltonianActivation, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        H = x.T @ x - x @ x.T\n",
    "        return H\n",
    "\n",
    "class SpectrumAnalysisActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpectrumAnalysisActivation, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        spectrum = torch.fft.fft2(x)\n",
    "        response_surface = torch.abs(spectrum)\n",
    "        return response_surface\n",
    "\n",
    "class HarmonicsDetectionActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HarmonicsDetectionActivation, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        harmonics = torch.fft.fft(x)\n",
    "        return harmonics.abs()\n",
    "\n",
    "class HarmonicsLyapunovActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HarmonicsLyapunovActivation, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        harmonics = torch.fft.fft(x)\n",
    "        lyapunov_exponent = torch.log(torch.abs(harmonics) + 1e-9)\n",
    "        return lyapunov_exponent\n",
    "\n",
    "class ResonanceFrequencyActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResonanceFrequencyActivation, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        stft = torch.stft(x, n_fft=128, hop_length=64)\n",
    "        return torch.abs(stft)\n",
    "\n",
    "class WeightedUndirectedNetworkActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WeightedUndirectedNetworkActivation, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        adjacency_matrix = x @ x.T\n",
    "        return adjacency_matrix\n",
    "\n",
    "class PhaseSpaceCentroidsActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PhaseSpaceCentroidsActivation, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        phase_space_2d = torch.stack((x[:, :-1], x[:, 1:]), dim=-1)\n",
    "        centroids = torch.mean(phase_space_2d, dim=1)\n",
    "        return centroids\n",
    "\n",
    "class MultipartiteConcurrenceActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultipartiteConcurrenceActivation, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        concurrence = torch.abs(x @ x.T.conj())\n",
    "        return concurrence\n",
    "\n",
    "class LaplaceTransformActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LaplaceTransformActivation, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        s = torch.fft.fft(x)\n",
    "        return torch.fft.ifft(s / (s + 1e-9))\n",
    "\n",
    "class SymplecticActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SymplecticActivation, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x @ torch.eye(x.size(1)).to(x.device)\n",
    "\n",
    "class HyperbolicActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HyperbolicActivation, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sinh(x)\n",
    "\n",
    "class GeodesicGaussianCurvatureActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GeodesicGaussianCurvatureActivation, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        curvature = 1 / (1 + torch.sum(x**2, dim=-1))\n",
    "        return curvature.unsqueeze(-1)\n",
    "\n",
    "class TheoremaEgregiumActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TheoremaEgregiumActivation, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x / torch.sqrt(1 + torch.sum(x**2, dim=-1)).unsqueeze(-1)\n",
    "\n",
    "class RiemannianActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RiemannianActivation, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.exp(-torch.norm(x, dim=-1)).unsqueeze(-1)\n",
    "\n",
    "class RiemannCurvatureActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RiemannCurvatureActivation, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Compute the gradient of the input tensor\n",
    "        gradient = torch.autograd.grad(x.sum(), x, create_graph=True)[0]\n",
    "\n",
    "        # Compute the Laplacian of the input tensor\n",
    "        laplacian = sum(torch.autograd.grad(gradient[:, i].sum(), x, create_graph=True)[0][:, i] for i in range(x.shape[1]))\n",
    "\n",
    "        # Compute the Riemannian Curvature-like quantity\n",
    "        riemann_curvature = torch.norm(laplacian)\n",
    "\n",
    "        # Apply a scaling factor to control the magnitude of the curvature\n",
    "        scaled_curvature = torch.tanh(riemann_curvature)  # You can experiment with different scaling functions\n",
    "\n",
    "        return scaled_curvature\n",
    "\n",
    "class JacobianActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(JacobianActivation, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        J = torch.autograd.functional.jacobian(lambda x: x, x)\n",
    "        return J\n",
    "\n",
    "class LyapunovVectorsActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LyapunovVectorsActivation, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.log(torch.abs(x) + 1e-9)\n",
    "\n",
    "class LyapunovStabilityActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LyapunovStabilityActivation, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.exp(-torch.abs(x))\n",
    "\n",
    "class LyapunovDimensionActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LyapunovDimensionActivation, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sum(torch.log(1 + torch.abs(x)), dim=-1).unsqueeze(-1)\n",
    "\n",
    "class OGYActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OGYActivation, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.tanh(x)\n",
    "\n",
    "class KolmogorovSinaiActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KolmogorovSinaiActivation, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.log(1 + torch.var(x, dim=-1)).unsqueeze(-1)\n",
    "\n",
    "class AmplitudeEnvelopeActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AmplitudeEnvelopeActivation, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        analytic_signal = torch.view_as_complex(torch.fft.fft(x))\n",
    "        envelope = torch.abs(analytic_signal)\n",
    "        return envelope\n",
    "\n",
    "class PyragasActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PyragasActivation, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sin(x) * torch.exp(-torch.abs(x))\n",
    "\n",
    "class SoulActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SoulActivation, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.tanh(x)\n",
    "\n",
    "class GrowthMeasureActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GrowthMeasureActivation, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.exp(-torch.abs(x))\n",
    "\n",
    "class BernoulliSchemeActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BernoulliSchemeActivation, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "class KakutaniActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KakutaniActivation, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cosh(x)\n",
    "\n",
    "class KakutaniActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KakutaniActivation, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cosh(x)\n",
    "\n",
    "class MinkowskiActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MinkowskiActivation, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sqrt(torch.abs(x))\n",
    "\n",
    "class RelativisticVelocityActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RelativisticVelocityActivation, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x / torch.sqrt(1 + x**2)\n",
    "\n",
    "class SpaceCurvatureActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpaceCurvatureActivation, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sin(x) / x\n",
    "\n",
    "class LieGroupActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LieGroupActivation, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.exp(x) - 1\n",
    "\n",
    "class MatrixLieGroupActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MatrixLieGroupActivation, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.matrix_exp(x)\n",
    "\n",
    "class ComplexManifoldActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ComplexManifoldActivation, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.view_as_real(x)\n",
    "\n",
    "class QuaternionicManifoldActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QuaternionicManifoldActivation, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.stack((x, -x), dim=-1)\n",
    "\n",
    "class HermitianManifoldActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HermitianManifoldActivation, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + x.T.conj()\n",
    "\n",
    "class BanachManifoldActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BanachManifoldActivation, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.norm(x, p=float('inf'))\n",
    "\n",
    "class FrechetManifoldActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FrechetManifoldActivation, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.norm(x, p=2)\n",
    "\n",
    "class HomomorphismActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HomomorphismActivation, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.exp(x)\n",
    "\n",
    "class InfinitesimalGeneratorActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(InfinitesimalGeneratorActivation, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + torch.eye(x.size(-1)).to(x.device)\n",
    "\n",
    "class EhresmannConnectionActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EhresmannConnectionActivation, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * torch.cosh(x)\n",
    "\n",
    "class VectorBundleActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VectorBundleActivation, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cov(x)\n",
    "\n",
    "class HolographicInformationActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HolographicInformationActivation, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply a 2D Fourier transform to represent holographic information\n",
    "        hologram = torch.fft.fft2(x)\n",
    "        \n",
    "        # Simulate encoding in a lower-dimensional boundary\n",
    "        boundary_encoding = hologram.mean(dim=-1)\n",
    "        \n",
    "        # Use a non-linear transformation to simulate information retrieval\n",
    "        retrieved_information = torch.tanh(boundary_encoding)\n",
    "        \n",
    "        # Transform back to the original domain\n",
    "        holographic_info = torch.fft.ifft2(retrieved_information)\n",
    "        \n",
    "        # Take the real part of the inverse FFT result\n",
    "        holographic_activation = torch.real(holographic_info)\n",
    "        \n",
    "        return holographic_activation\n",
    "\n",
    "class GaussianActivation(nn.Module):\n",
    "    def __init__(self, mu=0.0, sigma=1.0):\n",
    "        super(GaussianActivation, self).__init__()\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.exp(-0.5 * ((x - self.mu) / self.sigma)**2)\n",
    "\n",
    "class SincActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SincActivation, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.where(x == 0, torch.ones_like(x), torch.sin(x) / x)\n",
    "\n",
    "class PolynomialActivation(nn.Module):\n",
    "    def __init__(self, coefficients):\n",
    "        super(PolynomialActivation, self).__init__()\n",
    "        self.coefficients = coefficients\n",
    "\n",
    "    def forward(self, x):\n",
    "        result = torch.zeros_like(x)\n",
    "        for i, coeff in enumerate(self.coefficients):\n",
    "            result += coeff * (x ** i)\n",
    "        return result\n",
    "\n",
    "class WaveletActivation(nn.Module):\n",
    "    def __init__(self, wavelet='db1'):\n",
    "        super(WaveletActivation, self).__init__()\n",
    "        self.wavelet = wavelet\n",
    "\n",
    "    def forward(self, x):\n",
    "        import pywt\n",
    "        coeffs = pywt.wavedec(x.cpu().numpy(), self.wavelet)\n",
    "        return torch.tensor(coeffs[0]).to(x.device)\n",
    "\n",
    "class QuantumActivation(nn.Module):\n",
    "    def __init__(self, alpha=1.0):\n",
    "        super(QuantumActivation, self).__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sin(self.alpha * x) * torch.exp(-self.alpha * x**2)\n",
    "\n",
    "class LorentzActivation(nn.Module):\n",
    "    def __init__(self, c=1.0):\n",
    "        super(LorentzActivation, self).__init__()\n",
    "        self.c = c\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 1 / torch.sqrt(1 - (x / self.c)**2)\n",
    "\n",
    "class EntropyActivation(nn.Module):\n",
    "    def __init__(self, base=2):\n",
    "        super(EntropyActivation, self).__init__()\n",
    "        self.base = base\n",
    "\n",
    "    def forward(self, x):\n",
    "        p = F.softmax(x, dim=1)\n",
    "        log_p = torch.log(p + 1e-12) / torch.log(torch.tensor(self.base).float())\n",
    "        entropy = -torch.sum(p * log_p, dim=1, keepdim=True)\n",
    "        return x * entropy\n",
    "\n",
    "class HarmonicOscillatorActivation(nn.Module):\n",
    "    def __init__(self, omega=1.0):\n",
    "        super(HarmonicOscillatorActivation, self).__init__()\n",
    "        self.omega = omega\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cos(self.omega * x) * torch.exp(-0.5 * (x / self.omega)**2)\n",
    "\n",
    "class HarmonicsDetectionActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HarmonicsDetectionActivation, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        harmonics = torch.fft.fft(x)\n",
    "        return harmonics.abs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5591ee01-4ed0-4035-ad2d-286c730c01eb",
   "metadata": {},
   "source": [
    "# CNN Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98da8274-6e7c-4222-a843-1c861d722c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 32, 1])\n",
      "Labels shape: torch.Size([32])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given input size: (64x1x1). Calculated output size: (64x1x0). Output size is too small",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 90\u001b[0m\n\u001b[1;32m     87\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Save the model for future use\u001b[39;00m\n\u001b[1;32m     93\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvariance_classifier.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "Cell \u001b[0;32mIn[4], line 59\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     58\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 59\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     61\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "Cell \u001b[0;32mIn[4], line 33\u001b[0m, in \u001b[0;36mCustomActivationNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     32\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)\n\u001b[0;32m---> 33\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpool1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m     36\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/pooling.py:88\u001b[0m, in \u001b[0;36mMaxPool1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_indices\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/_jit_internal.py:365\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:603\u001b[0m, in \u001b[0;36m_max_pool1d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    602\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[0;32m--> 603\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given input size: (64x1x1). Calculated output size: (64x1x0). Output size is too small"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class CustomActivationNet(nn.Module):\n",
    "    def __init__(self, input_channels=32, input_length=125):\n",
    "        super(CustomActivationNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_channels, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2)\n",
    "        self.conv3 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=2)\n",
    "\n",
    "        # Initialize custom activations with their respective filepaths\n",
    "        self.activation1 = PeakHeightActivation(os.path.join(data_dir, 'peak_height_250ms_500hz_tensor.pt'))\n",
    "        self.activation2 = PeakCountsActivation(os.path.join(data_dir, 'peak_counts_250ms_500hz_tensor.pt'))\n",
    "        self.activation3 = AveragePeakHeightActivation(os.path.join(data_dir, 'average_peak_height_250ms_500hz_tensor.pt'))\n",
    "        self.activation4 = AverageDistanceActivation(os.path.join(data_dir, 'average_distance_250ms_500hz_tensor.pt'))\n",
    "        self.activation5 = AverageProminenceActivation(os.path.join(data_dir, 'average_prominence_250ms_500hz_tensor.pt'))\n",
    "        self.activation6 = VarianceActivation(os.path.join(data_dir, 'variance_250ms_500hz_tensor.pt'))\n",
    "        self.activation7 = StdDevActivation(os.path.join(data_dir, 'std_dev_250ms_500hz_tensor.pt'))\n",
    "        self.activation8 = RMSActivation(os.path.join(data_dir, 'rms_250ms_500hz_tensor.pt'))\n",
    "        #self.activation9 = FrequenciesActivation(os.path.join(data_dir, 'frequencies_250ms_500hz_tensor.pt'))\n",
    "        self.activation9 = SpectralEntropyActivation(os.path.join(data_dir, 'spectral_entropy_250ms_500hz_tensor.pt'))\n",
    "        \n",
    "        self.fc1 = nn.Linear(256 * (input_length // 8), 512)\n",
    "        self.fc2 = nn.Linear(512, 1)  # Output layer for binary classification\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.pool3(x)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        # Flatten the output for the fully connected layer\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels.unsqueeze(1))\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {loss.item():.4f}, Validation Loss: {val_loss/len(val_loader):.4f}')\n",
    "\n",
    "# Example training loop\n",
    "input_size = 32  # Assuming each feature vector has 32 elements (num_channels)\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "\n",
    "for inputs, labels in train_loader:\n",
    "    print(\"Input shape:\", inputs.shape)\n",
    "    print(\"Labels shape:\", labels.shape)\n",
    "    break\n",
    "\n",
    "# Initialize model, criterion, and optimizer\n",
    "model = CustomActivationNet(input_channels=input_size, input_length=125)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs)\n",
    "\n",
    "# Save the model for future use\n",
    "torch.save(model.state_dict(), os.path.join(save_dir, 'variance_classifier.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee34df59-9462-42f2-8b0e-d684ea2011de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
